
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }
 
    .box{
        width:1020px;
        margin:0 auto;
        position:relative;
    }
    .box .left{
        width:340px;
        /* height:300px; */
        /* background-color:#eee; */
        position:absolute;
       
    }
    .box .center{
        width:340px;
        /* height:300px; */
        /* background-color:#eee; */
        margin:0 auto;
    }
    .box .right{
        width:340px;
        /* height:300px; */
        /* background-color:#eee; */
        position:absolute;
        right:0;
        top:0;
 
    }
    .box_two{
        width:900px;
        margin:0 auto;
        position:relative;
    }
    .box_two .left{
        width:450px;
        /* height:300px; */
        /* background-color:#eee; */
        position:absolute;
       
    }

    .box_two .right{
        width:450px;
        /* height:300px; */
        /* background-color:#eee; */
        position:absolute;
        right:0;
        top:0;
 
    }


    h1 {
        font-weight:300;
        margin: 0.4em;
    }

    p {
        margin: 0.2em;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    /* 图片的重叠 */

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        margin: 0;
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
    <head>
        <title>Diff-HMR</title>
    </head>
    

    <body>
	    
        <br>
        <center>
            <span style="font-size:42px">Diff-HMR: Toward more reliable human pose and shape estimation from video</span>
        </center>
        <br>
	    
	<br> <!-- 换行-->
   
        <!-- framework -->
        <table align=center width=900px>
            	<tr>
	        	<td>
                    		<center>                      
		        		<img src = "./framework_update.png" width="1080px"></img>
                        		<span style="font-size:15px;font-style:italic">    The overview of our proposed framework. </span>
                    		</center>
                	</td> 
	    	</tr>
        </table>
        <!-- abstract-->
    	<table align=center width=900px></table>
            <tr>
                <td width=600px>
                    <br>
                    <p align="justify" style="font-size: 18px">
                        The accurate recovery of 3D human shape and pose from monocular videos is crucial for understanding human behavior in various media. However, current methods have yet to address the uncertainties associated with 3D estimation, such as inherent ambiguity, blurring, and occlusion, resulting in inaccurate estimates, particularly in complex scenarios.  To overcome this challenge, we propose a novel Diffusion-based Human Mesh Recovery (Diff-HMR) network, inspired by the powerful capabilities of the diffusion model in generation tasks. Specifically, our approach incorporates a Diffusion-based pose regressor (DPR) that models the estimation of a deterministic pose as a reverse diffusion process, while a Cross-Temporal Attention Module (CTAM) aggregates contextual features as a condition for the reverse diffusion process. The ground truth pose parameters are used to supervise process values through a stepwise diffusion. Additionally, we introduce a vertex constraint loss to provide more precise constraints on the final output of the human mesh. Experimental results demonstrate that our proposed Diff-HMR outperforms state-of-the-art methods on two benchmark datasets, 3DPW and Human3.6M, and provides better handling of uncertainty in human pose and shape estimation.    
                    </td>
	    </tr>
	
    </br>
    </div> 
    </br>
    <hr>   
    <center><h1>Visual comparison of Diff-HMR with other state-of-the-art methods </h1></center>
    
    <!-- demo comparison-->
    <div class="box">
        <div class="left">
            <video id="video2" width="320" style= "margin-top:10px; margin-right:20px;">
                <source src="mps_han_short_output.mp4" type="video/mp4" />
            </video>                
        </div>
        <div class="center">
            <video id="video1" width="320" style= "margin-top:10px;">
                <source src="DIFF_han_short_output.mp4" type="video/mp4" />
            </video>
        </div>
        <div class="right">
            <video id="video3" width="320" style= "margin-top:10px; margin-right:20px;">
                <source src="tcmr_han_short_output.mp4" type="video/mp4" />
            </video>
        </div>
    </div> 
    <center><button onclick= "play_Pause();" style= "margin-top:10px;">play/pause</button></center>

    <script type="text/javascript">
    var myVideo=document.getElementById ("video1");
    var myVideo2=document.getElementById ("video2");
    var myVideo3=document.getElementById ("video3");
    function play_Pause() {
        if (myVideo.paused){
            myVideo.play();
            myVideo2.play();
            myVideo3.play();
        }
        else{
            myVideo.pause();
            myVideo2.pause();
            myVideo3.pause();
        }
    }
    </script>
    
    </br>
    <center>
    <span style="font-size:14px">  Given a challenging video which contains fast motion, self occlusion and blurring, Diff-HMR(middle) predicts more accurate result
        than TCMR [4] (right) and MPS-Net [40] (left). In addition, it provides more accurate prediction of the people’s orientation.
     </span>
    </center>
    
    </br>
    <!-- 3DPW comparison-->
    <div class="box">
        <div class="left">
            <video id="video_l" width="320" style= "margin-top:10px; margin-right:20px;">
                <source src="mps_slalom_output.mp4" type="video/mp4" />
            </video>                
        </div>
        <div class="center">
            <video id="video_c" width="320" style= "margin-top:10px;">
                <source src="diff_slalom_output.mp4" type="video/mp4" />
            </video>
        </div>
        <div class="right">
            <video id="video_r" width="320" style= "margin-top:10px; margin-right:20px;">
                <source src="tcmr_slalom_output.mp4" type="video/mp4" />
            </video>
        </div>
    </div> 
    <center><button onclick= "play_Pause_2();" style= "margin-top:10px;">play/pause</button></center>

    <script type="text/javascript">
    var myVideo_l=document.getElementById ("video_l");
    var myVideo_c=document.getElementById ("video_c");
    var myVideo_r=document.getElementById ("video_r");
    function play_Pause_2() {
        if (myVideo_c.paused){
            myVideo_l.play();
            myVideo_c.play();
            myVideo_r.play();
        }
        else{
            myVideo_l.pause();
            myVideo_c.pause();
            myVideo_r.pause();
        }
    }
    </script>

    </br>
    <!-- Description-->    
    </tr>
        <center>
	    <span style="font-size:14px">   
            Qualitative comparison between Diff-HMR(middle), MPS-Net[40](left) and TCMR[4](right) on the challenging in-the-wild dataset 3DPW[38]. This video contains occlusionand fast motion. Diff-HMR produces better results
            than the other two.    
        </span>
 	</center>
</br>
<hr>
<table align=center width=900px>
            <center><h1>Demo Videos (MPS-Net)</h1></center>
            <tr>
	        <center>
                    <a href="./mp4_compare.mp4">
                        <video width="900" controls preload>
                            <source src="./mp4_compare.mp4" type="video/mp4">
                        </video>
                    </a><br>	
		    <span style="font-size:16px"> Original video frame rate: 24.72fps </span>
                </center>		
		<br>
                <center>
                    <a href="./demo_compare.mp4">
                        <video width="900" controls preload>
                            <source src="./demo_compare.mp4" type="video/mp4">
                        </video>
                    </a><br>
		    <span style="font-size:16px"> Original video frame rate: 29.97fps </span>
                </center>
		<br>
                <center>
                    <a href="./mp3_compare.mp4">
                        <video width="900" controls preload>
                            <source src="./mp3_compare.mp4" type="video/mp4">
                        </video>
                    </a><br>
		    <span style="font-size:16px"> Original video frame rate: 24.72fps </span>
                </center> 
		<br>
		<center>
                    <a href="./visual2_compare.mp4">
                        <video width="500" controls preload>
                            <source src="./visual2_compare.mp4" type="video/mp4">
                        </video>
                    </a><br>
		    <span style="font-size:16px"> Original video frame rate: 24.79fps </span>
                </center>

        <br>
		<center>
                    <a href="./visual1_compare.mp4">
                        <video width="500" controls preload>
                            <source src="./visual1_compare.mp4" type="video/mp4">
                        </video>
                    </a><br>
		    <span style="font-size:16px"> Original video frame rate: 24.68fps </span>
                </center>
	    </tr>
    </table>
    
    <br>
    <hr> 
        <table align=center width=900px>
            <center><h1>Visual effects of Diff-HMR in alternative viewpoints</h1></center>
            <tr>
                    <center>
                        <a href="./visual2.png"><img src = "./visual2.png" width="900px"></img></a><br>
                    </center><br>
		    <center>
                        <a href="./visual3.png"><img src = "./visual3.png" width="900px"></img></a><br>
                    </center><br>
		    <center>
                        <!-- <a href="./LSPchoice360.gif"><img src = "./LSPchoice360.gif" width="900px"></img></a><br>
		        <span style="font-size:14px">   <b>We visualize the 3D human body estimated by our MPS-Net from different viewpoints.</b> The results show that our MPS-Net is able to estimate the correct global body rotation. </span>
                    </center> -->
            </tr>
        </table>  
        
        
    
	<br>
        <hr>
        <center><h1>Qualitative Results</h1></center>
             <tr>
                 <th colspan='4'>
                   <center>
                     <span style="font-size:22px">Details of visual comparison between Diff-HMR (middle), MPS-Net (top) and TCMR (bottom)</span>
                   </center>
                 </th>
             </tr>
        <br>
        <table align=center width=900px>
        	<tr>
			<center>
				<a href="./compare_1.png"><img src = "./compare_1.png" width="900px"></img></a><br>
                                <!--<span style="font-size:14px">
                     			Qualitative comparison of TCMR [6] (left) and our MPS-Net (right) on the challenging in-the-wild 3DPW [37] and MPI-INF-3DHP [27] datasets.
                    		</span>-->
                    	</center><br>
		</tr>              
	
	</table>
        <br>
        <hr>
        <center><h1> Quantitative Results</h1></center> 
	<br>
        <table align=center width=900px>
            <tr>
                <td>
               <center>
                    <span style="font-size:18px">
                        <!--<b>Evaluation of state-of-the-art video-based methods on <a href='http://virtualhumans.mpi-inf.mpg.de/3DPW/'>3DPW</a>, <a href='http://vcai.mpi-inf.mpg.de/3dhp-dataset/'>MPI-INF-3DHP</a>, and <a href='http://vision.imar.ro/human3.6m/description.php'>Human3.6M</a> datasets.</b> Following Choi et al. [6], all methods are trained on the training set including 3DPW, but do not use the Human3.6M SMPL parameters obtained from <a href='https://files.is.tue.mpg.de/black/papers/MoSh.pdf'>Mosh</a>. The number of input frames follows the original protocol of each method.-->
                        <b>Table 1. Evaluation of state-of-the-art methods on 3DPW [37] and Human3.6M [16] datasets.</b> 
			<!--<b>Chest X-ray Lung Segmentation</b> Numbers are DICE scores. 
                        <a href='http://db.jsrt.or.jp/eng.php'>JSRT</a> is the in-domain dataset, 
                        on which we both train and evaluate. We also evaluate on additional out-of-domain datasets 
                        (<a href='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4256233/'>NLM</a>, 
                        <a href='https://arxiv.org/abs/1803.01199'>NIH</a>, 
                        <a href='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4256233/'>SZ</a>). 
                        Ours as well as the other semi-supervised methods use additional 108k unlabeled data samples.-->
                    </span>
                </center>
                </td>                
            </tr>
            <tr>
                <td width=100px>
                    <center>
                        <a href="./result_1.png"><img src = "./result_1.png" width="1000px"></img></a><br>
                    </center>
                </td>
	    </tr>                
        </table>
        <br>
        <table align=center width=900px>
            <tr>
                <td>
		  <center>
                    <span style="font-size:18px">
                        <!--<b>Comparison of the number of network parameters</b>-->
			<b>Table 2. Comparison of the number of network parameters, FLOPs, and model size.</b>
                    </span>
		  </center>
                </td>                
            </tr>
            <tr>
                <td width=300px>
                    <center>
                        <a href="./result_2.png"><img src = "./result_2.png" width="500px"></img></a><br>
                    </center>
                </td>
	    </tr>                
        </table>
        
        <br>
        <hr>
        <br>
        <table align=center width=900px>
            <tr>
                <td>
                <center>
                    <span style="font-size:18px">
                        <!--<b>Ablation study for different modules of the proposed MPS-Net on the 3DPW [37] dataset.</b> The training and evaluation settings are the same as he above table.-->
                        <b>Table 3. Ablation study on the impact of DPR on the 3DPW [37] dataset.</b>
                        <!--<b>CT-MRI Transfer Liver Segmentation</b> Numbers are DICE per patient. CT
                        is the in-domain data set on which we both train and evaluate. We also evaluate on additional unseen 
                        MRI T1-in and MRI T1-out from
                        <a href='https://chaos.grand-challenge.org/'>CHAOS</a> dataset. 
                        Ours as well as the semi-supervised methods use additional 70 CT volumes from the 
                        <a href='https://competitions.codalab.org/competitions/17094'>LITS2017</a> testing set as unlabeled data samples for training.-->
                    </span>
                </center>
                </td>                
            </tr>
            <tr>
                <td width=100px>
                    <center>
                        <a href="./result_3.png"><img src = "./result_3.png" width="600px"></img></a><br>
                    </center>
                </td>
            </tr>
        </table>
        <br>
        <hr>
        <br>
        <table align=center width=900px>
            <tr>
                <td>
                    <center>
                    <span style="font-size:18px">
                        <!--<b>Evaluation of state-of-the-art single image-based and video-based methods on 3DPW [37] dataset.</b> All methods do not use 3DPW in training.-->
			<b>Table 5. Ablation study on the impact of attention module on the performance on the 3DPW [37] dataset.</b>
                    </span>
                </center>
                </td>                
            </tr>
            <tr>
                <td width=100px>
                    <center>
                        <a href="./result_4.png"><img src = "./result_4.png" width="600px"></img></a><br>
                    </center>
                </td>
            </tr>
        </table>
        <br>
        <br>
        <hr>
        <table align=center width=900px>
            <tr>
                <td>
                    <center>
                    <span style="font-size:18px">
                        <!--<b>Evaluation of state-of-the-art single image-based and video-based methods on 3DPW [37] dataset.</b> All methods do not use 3DPW in training.-->
			<b>Table 6. Ablation study on the impact of the loss on the performance on the 3DPW [37] dataset.</b> 
                    </span>
                </center>
                </td>                
            </tr>
            <tr>
                <td width=100px>
                    <center>
                        <a href="./result_5.png"><img src = "./result_5.png" width="600px"></img></a><br>
                    </center>
                </td>
            </tr>
        </table>
        <br>
        <br>
        <hr>
 	<a name="related_work"></a>
 	<table align=center width=1100px>
 	    <tr>
		<td width=400px>
 			<left>
  		  	<center><h1>References</h1></center>
 		 	<br> [4] Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee. 2021. <b>Beyond static features for temporally consistent 3d human pose and shape from a video.</b> In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1964–1973.</br> 
            <br> [15] Angjoo Kanazawa, Michael J Black, David W Jacobs, and Jitendra Malik. 2018. <b> End-to-end recovery of human shape and pose.</b> In Proceedings of the IEEE conference on computer vision and pattern recognition. 7122–7131.</br>
            <br> [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. <b>Denoising diffusion probabilistic models.</b> Advances in Neural Information Processing Systems 33 (2020), 6840–6851. </br>
            <br> [16] Angjoo Kanazawa, Jason Y Zhang, Panna Felsen, and Jitendra Malik. 2019. <b>Learning 3d human dynamics from video.</b> In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 5614–5623.</br> 
            <br> [21] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis. 2019. <b>Learning to reconstruct 3D human pose and shape via model-fitting in the loop.</b> In Proceedings of the IEEE/CVF international conference on computer vision. 2252–2261. </br>
            <br> [37] Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, Yili Fu, and Tao Mei. 2019. <b>Human mesh recovery from monocular images via a skeleton-disentangled representation.</b> In Proceedings of the IEEE/CVF international conference on computer vision. 5349–5358.</br> 
            <br> [39] Ziniu Wan, Zhengjia Li, Maoqing Tian, Jianbo Liu, Shuai Yi, and Hongsheng Li. 2021. <b>Encoder-decoder with multi-level attention for 3D human shape and pose estimation.</b> In Proceedings of the IEEE/CVF International Conference on Computer Vision. 13033–13042.</br>
            <br> [40] Wen-Li Wei, Jen-Chun Lin, Tyng-Luh Liu, and Hong-Yuan Mark Liao. 2022. <b>Capturing humans in motion: temporal-attentive 3D human pose and shape estimation from monocular video.</b> In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 13211–13220.</br>
		</td>
	    </tr>
	</table>
	<br>
        <br>
        <!-- <hr> -->
        <!--<a name="related_work"></a>-->
 	<!-- <table align=center width=1100px>
 	    <tr>
		<td width=400px> -->
 			<!--<left>-->
  		  	<!-- <center>
				<h1>Contact Information</h1>
				Wen-Li Wei, Jen-Chun Lin {lilijinjin@gmail.com; jenchunlin@gmail.com}
			</center>
		</td>
	    </tr>
	</table> -->
	<br>
        <br>
        <hr>
	<!-- <table align=center width=1100px>
 	    <tr>
		<td width=400px> -->
 			<!--<left>-->
  		  	<!-- <center>
				<h1>Acknowledgements</h1>
				This webpage template was refered from Phillip Isola and Richard Zhang. Thanks a lot.
			</center>
		</td>
	    </tr>
	</table> -->
    </body>
</html>
